{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vUqn6fkhqdQU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 1: 10 Classifiers with logistic regression and MSE"
      ]
    },
    {
      "metadata": {
        "id": "f7RAJZovEuWf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import image data"
      ]
    },
    {
      "metadata": {
        "id": "CF3GDc3L3vPC",
        "colab_type": "code",
        "outputId": "891f3a4d-baac-410b-ef55-1dd196f8a1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "G90d4jBiE1X_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing image data and hyperparameters\n",
        "\n",
        "Problem 1 has been giving me trouble, not in its difficulty, but in tuning the hyperparameters.  The batch size I have attempted to keep at 128 per the example we have studied in class, but the learning rate seems to need to go to relatively huge levels to get any decent accuracy.  I have narrowed this issue down to the partial derivatives of the loss function, but as the math is absurdly simple, and I have verified that it is correct, I can only assume that this problem is due to the fact that creating 10 classifiers with mean squared error is bound to produce mediocre results."
      ]
    },
    {
      "metadata": {
        "id": "OefQa4xdSHfP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "lr = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GwYyKegIFvNZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Function for converting labels to binary representation\n",
        "\n",
        "This vectorized function takes each label in a list of labels, and, depending on the classifier being made, converts it to a binary label.  So if the classifier is for recognizing the number 2, then each label corresponding to 2 will be converted to 1 and all other labels are 0."
      ]
    },
    {
      "metadata": {
        "id": "e9uG-xMmSPgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_labels(old_label, new_label):\n",
        "  return 1 if old_label == new_label else 0\n",
        "convert_labels = np.vectorize(convert_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G74vtgUFGNkV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic regression activation function\n",
        "\n",
        "This function takes each element in a list of weighted inputs and passes it to the sigmoid."
      ]
    },
    {
      "metadata": {
        "id": "SrGWXTergHqw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def activation(z):\n",
        "  return 1/(1 + np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNJ8pP6sHtza",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mean-squared-error loss function\n",
        "\n",
        "This function takes the mean squared error between the activation result and the label."
      ]
    },
    {
      "metadata": {
        "id": "JZjxOsmdHsTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(a,y):\n",
        "  return 0.5*(a-y)**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWUc-uunG54y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier for each number\n",
        "\n",
        "This classifier takes as input the number we will be training to classify.  It then converts all labels to binary representations for that number, and trains the network.  We do this for each number from 0 to 9."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "41C7F6yLGDau",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1972
        },
        "outputId": "15b6a60c-d7ae-4b8a-b473-2978d068d0f5"
      },
      "cell_type": "code",
      "source": [
        "def classifier(num):\n",
        "  # Convert labels to binary representation for number being trained.\n",
        "  train_labels = convert_labels(train_labels_original, num)\n",
        "  # Initialize random weights and bias.\n",
        "  weights = np.random.randn(784)\n",
        "  bias = np.random.randn()\n",
        "  \n",
        "  num_ims = len(train_images)\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    # Shuffle the training data on each epoch.\n",
        "    shuffled_i = np.random.permutation(num_ims)\n",
        "    X = train_images[shuffled_i]\n",
        "    Y = train_labels[shuffled_i]\n",
        "    \n",
        "    # Calculate the average loss on each epoch.\n",
        "    loss_epoch = 0.0\n",
        "    for i in range(0, num_ims, batch_size):\n",
        "        x = X[i:i+batch_size]\n",
        "        y = Y[i:i+batch_size]\n",
        "        \n",
        "        # Take each training example in the batch and dot it with the\n",
        "        # weights and then add the bias to get the weighted input.\n",
        "        z = x.dot(weights) + bias\n",
        "        # Pass the weighted input to the activation.\n",
        "        a = activation(z)\n",
        "        \n",
        "        # Calculate the loss for this example and add it to loss for the epoch.\n",
        "        loss_epoch += np.sum(loss(a, y))\n",
        "        \n",
        "        # Calculate the gradients with respect to the weights and bias\n",
        "        # for each example.  \n",
        "        gradient_b = (a - y) * a * (1.0 - a)\n",
        "        \n",
        "        # We need to reshape so that the vectorized operation for the gradient\n",
        "        # is done correctly.\n",
        "        gradient_b = np.reshape(gradient_b, (len(gradient_b),1))\n",
        "        gradient_w = x * gradient_b\n",
        "       \n",
        "        # Average the gradients across the batch and use the resulting value in the updates.\n",
        "        gradient_b = np.mean(gradient_b)\n",
        "        gradient_w = np.mean(gradient_w,axis=0)\n",
        "        \n",
        "        weights = weights - lr * gradient_w\n",
        "        bias = bias - lr * gradient_b\n",
        "    \n",
        "    # Record the progress of the current classifier.\n",
        "    print(\"Classifier %d: Epoch %d/%d.\\tLoss: %f\" % (num, epoch+1, epochs, loss_epoch/num_ims))\n",
        "  print()\n",
        "  \n",
        "  return (weights, bias)\n",
        "\n",
        "# Train a list of classifiers, one for each digit.\n",
        "classif_list = [classifier(i) for i in range(10)]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier 0: Epoch 1/10.\tLoss: 0.049679\n",
            "Classifier 0: Epoch 2/10.\tLoss: 0.049391\n",
            "Classifier 0: Epoch 3/10.\tLoss: 0.049371\n",
            "Classifier 0: Epoch 4/10.\tLoss: 0.049352\n",
            "Classifier 0: Epoch 5/10.\tLoss: 0.049330\n",
            "Classifier 0: Epoch 6/10.\tLoss: 0.049302\n",
            "Classifier 0: Epoch 7/10.\tLoss: 0.049225\n",
            "Classifier 0: Epoch 8/10.\tLoss: 0.034694\n",
            "Classifier 0: Epoch 9/10.\tLoss: 0.009687\n",
            "Classifier 0: Epoch 10/10.\tLoss: 0.006896\n",
            "\n",
            "Classifier 1: Epoch 1/10.\tLoss: 0.053533\n",
            "Classifier 1: Epoch 2/10.\tLoss: 0.023145\n",
            "Classifier 1: Epoch 3/10.\tLoss: 0.007168\n",
            "Classifier 1: Epoch 4/10.\tLoss: 0.005894\n",
            "Classifier 1: Epoch 5/10.\tLoss: 0.005260\n",
            "Classifier 1: Epoch 6/10.\tLoss: 0.004848\n",
            "Classifier 1: Epoch 7/10.\tLoss: 0.004550\n",
            "Classifier 1: Epoch 8/10.\tLoss: 0.004321\n",
            "Classifier 1: Epoch 9/10.\tLoss: 0.004133\n",
            "Classifier 1: Epoch 10/10.\tLoss: 0.004018\n",
            "\n",
            "Classifier 2: Epoch 1/10.\tLoss: 0.051616\n",
            "Classifier 2: Epoch 2/10.\tLoss: 0.049712\n",
            "Classifier 2: Epoch 3/10.\tLoss: 0.049679\n",
            "Classifier 2: Epoch 4/10.\tLoss: 0.049665\n",
            "Classifier 2: Epoch 5/10.\tLoss: 0.049658\n",
            "Classifier 2: Epoch 6/10.\tLoss: 0.049654\n",
            "Classifier 2: Epoch 7/10.\tLoss: 0.049652\n",
            "Classifier 2: Epoch 8/10.\tLoss: 0.049650\n",
            "Classifier 2: Epoch 9/10.\tLoss: 0.049649\n",
            "Classifier 2: Epoch 10/10.\tLoss: 0.049648\n",
            "\n",
            "Classifier 3: Epoch 1/10.\tLoss: 0.053357\n",
            "Classifier 3: Epoch 2/10.\tLoss: 0.051175\n",
            "Classifier 3: Epoch 3/10.\tLoss: 0.051137\n",
            "Classifier 3: Epoch 4/10.\tLoss: 0.051119\n",
            "Classifier 3: Epoch 5/10.\tLoss: 0.051109\n",
            "Classifier 3: Epoch 6/10.\tLoss: 0.051102\n",
            "Classifier 3: Epoch 7/10.\tLoss: 0.051096\n",
            "Classifier 3: Epoch 8/10.\tLoss: 0.051092\n",
            "Classifier 3: Epoch 9/10.\tLoss: 0.051090\n",
            "Classifier 3: Epoch 10/10.\tLoss: 0.051088\n",
            "\n",
            "Classifier 4: Epoch 1/10.\tLoss: 0.026349\n",
            "Classifier 4: Epoch 2/10.\tLoss: 0.013759\n",
            "Classifier 4: Epoch 3/10.\tLoss: 0.011466\n",
            "Classifier 4: Epoch 4/10.\tLoss: 0.010286\n",
            "Classifier 4: Epoch 5/10.\tLoss: 0.009556\n",
            "Classifier 4: Epoch 6/10.\tLoss: 0.009072\n",
            "Classifier 4: Epoch 7/10.\tLoss: 0.008669\n",
            "Classifier 4: Epoch 8/10.\tLoss: 0.008354\n",
            "Classifier 4: Epoch 9/10.\tLoss: 0.008110\n",
            "Classifier 4: Epoch 10/10.\tLoss: 0.007901\n",
            "\n",
            "Classifier 5: Epoch 1/10.\tLoss: 0.042828\n",
            "Classifier 5: Epoch 2/10.\tLoss: 0.024138\n",
            "Classifier 5: Epoch 3/10.\tLoss: 0.018132\n",
            "Classifier 5: Epoch 4/10.\tLoss: 0.015876\n",
            "Classifier 5: Epoch 5/10.\tLoss: 0.014763\n",
            "Classifier 5: Epoch 6/10.\tLoss: 0.014060\n",
            "Classifier 5: Epoch 7/10.\tLoss: 0.013535\n",
            "Classifier 5: Epoch 8/10.\tLoss: 0.013174\n",
            "Classifier 5: Epoch 9/10.\tLoss: 0.012790\n",
            "Classifier 5: Epoch 10/10.\tLoss: 0.012514\n",
            "\n",
            "Classifier 6: Epoch 1/10.\tLoss: 0.051251\n",
            "Classifier 6: Epoch 2/10.\tLoss: 0.049365\n",
            "Classifier 6: Epoch 3/10.\tLoss: 0.049346\n",
            "Classifier 6: Epoch 4/10.\tLoss: 0.049340\n",
            "Classifier 6: Epoch 5/10.\tLoss: 0.049337\n",
            "Classifier 6: Epoch 6/10.\tLoss: 0.049335\n",
            "Classifier 6: Epoch 7/10.\tLoss: 0.049333\n",
            "Classifier 6: Epoch 8/10.\tLoss: 0.049332\n",
            "Classifier 6: Epoch 9/10.\tLoss: 0.049330\n",
            "Classifier 6: Epoch 10/10.\tLoss: 0.049329\n",
            "\n",
            "Classifier 7: Epoch 1/10.\tLoss: 0.054988\n",
            "Classifier 7: Epoch 2/10.\tLoss: 0.052237\n",
            "Classifier 7: Epoch 3/10.\tLoss: 0.052190\n",
            "Classifier 7: Epoch 4/10.\tLoss: 0.052104\n",
            "Classifier 7: Epoch 5/10.\tLoss: 0.051716\n",
            "Classifier 7: Epoch 6/10.\tLoss: 0.036134\n",
            "Classifier 7: Epoch 7/10.\tLoss: 0.012266\n",
            "Classifier 7: Epoch 8/10.\tLoss: 0.010062\n",
            "Classifier 7: Epoch 9/10.\tLoss: 0.009180\n",
            "Classifier 7: Epoch 10/10.\tLoss: 0.008716\n",
            "\n",
            "Classifier 8: Epoch 1/10.\tLoss: 0.049722\n",
            "Classifier 8: Epoch 2/10.\tLoss: 0.048827\n",
            "Classifier 8: Epoch 3/10.\tLoss: 0.048794\n",
            "Classifier 8: Epoch 4/10.\tLoss: 0.048785\n",
            "Classifier 8: Epoch 5/10.\tLoss: 0.048779\n",
            "Classifier 8: Epoch 6/10.\tLoss: 0.048776\n",
            "Classifier 8: Epoch 7/10.\tLoss: 0.048773\n",
            "Classifier 8: Epoch 8/10.\tLoss: 0.048771\n",
            "Classifier 8: Epoch 9/10.\tLoss: 0.048769\n",
            "Classifier 8: Epoch 10/10.\tLoss: 0.048768\n",
            "\n",
            "Classifier 9: Epoch 1/10.\tLoss: 0.035820\n",
            "Classifier 9: Epoch 2/10.\tLoss: 0.023273\n",
            "Classifier 9: Epoch 3/10.\tLoss: 0.019860\n",
            "Classifier 9: Epoch 4/10.\tLoss: 0.018341\n",
            "Classifier 9: Epoch 5/10.\tLoss: 0.017368\n",
            "Classifier 9: Epoch 6/10.\tLoss: 0.016606\n",
            "Classifier 9: Epoch 7/10.\tLoss: 0.015970\n",
            "Classifier 9: Epoch 8/10.\tLoss: 0.015437\n",
            "Classifier 9: Epoch 9/10.\tLoss: 0.015101\n",
            "Classifier 9: Epoch 10/10.\tLoss: 0.014766\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L-wbRGCqK5G_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prediction function\n",
        "\n",
        "This function takes a trained model for a classifier and a test example, and calculates the probability that the example is the number that the model is trained to recognize."
      ]
    },
    {
      "metadata": {
        "id": "e78p-i-lZ4rA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(model, x):\n",
        "  weights = model[0]\n",
        "  bias = model[1]\n",
        "  \n",
        "  z = x.dot(weights) + bias\n",
        "  return activation(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9IQsbMBRLKfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Checking accuracy of each classifier\n",
        "\n",
        "For each classifier, we pass it into a function that will determine how accurate the classifier is at recognizing its number by testing it on the entire test data set."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b4171809-1697-4cd1-80ce-bd7cabb2e5d2",
        "id": "kaDaCRNfxAPP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "def single_classf_acc(num):\n",
        "  num_true = 0\n",
        "  \n",
        "  # Convert all test labels to their binary form for the number\n",
        "  # being recognized.\n",
        "  test_labels = convert_labels(test_labels_original, num)\n",
        "  \n",
        "  for i in range(len(test_images)):\n",
        "    xi = test_images[i]\n",
        "    yi = test_labels[i]\n",
        "    y_pred = predict(classif_list[num], xi)\n",
        "    \n",
        "    if y_pred>=0.5:\n",
        "      y_pred = 1\n",
        "    else:\n",
        "      y_pred = 0\n",
        "\n",
        "    if yi == y_pred:\n",
        "      num_true+=1\n",
        "\n",
        "  print(\"Classifier %d Accuracy: %f\" % (num, num_true/len(test_images)))\n",
        "\n",
        "print(\"Individual Accuracies of Classifiers:\")\n",
        "x = [single_classf_acc(i) for i in range(10)]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Individual Accuracies of Classifiers:\n",
            "Classifier 0 Accuracy: 0.986100\n",
            "Classifier 1 Accuracy: 0.992300\n",
            "Classifier 2 Accuracy: 0.896800\n",
            "Classifier 3 Accuracy: 0.898900\n",
            "Classifier 4 Accuracy: 0.977700\n",
            "Classifier 5 Accuracy: 0.973400\n",
            "Classifier 6 Accuracy: 0.904200\n",
            "Classifier 7 Accuracy: 0.980700\n",
            "Classifier 8 Accuracy: 0.902600\n",
            "Classifier 9 Accuracy: 0.964700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_sJwUfiBL51K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Finding the accuracy of being able to recognize any number\n",
        "\n",
        "Using the list of classifiers, pass the test data to them, and using argmax, determine which number each example is by choosing the classifier that \"screams hardest\".  Ouput how accurately the classifiers do this."
      ]
    },
    {
      "metadata": {
        "id": "hvJaDgxTc8tM",
        "colab_type": "code",
        "outputId": "8938bc53-43f5-4f00-bf0d-3720391bf6c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "num_true = 0\n",
        "for i in range(len(test_images)):\n",
        "  xi = test_images[i]\n",
        "  yi = test_labels_original[i]\n",
        "  y_pred = np.argmax([predict(classif_list[i], xi) for i in range(10)])\n",
        "  \n",
        "  if yi == y_pred:\n",
        "    num_true+=1\n",
        "    \n",
        "acc = num_true/len(test_images)\n",
        "print(\"Overall Recognization Accuracy: %f\" % acc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall Recognization Accuracy: 0.564300\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}