{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_3",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ww2F1MXawpqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Classifier using softmax and categorical cross-entropy"
      ]
    },
    {
      "metadata": {
        "id": "r_dcGmK3xBZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import image data"
      ]
    },
    {
      "metadata": {
        "id": "baUdjSP4aboX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wUxXRH_MxTb6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing image data and hyperparameters\n",
        "\n",
        "Smaller batch sizes will obviously give better results, but I found that in fact, smaller batch sizes lead to overfitting, which reduces the accuracy of the classifier in the end.  A batch size of 32 was selected because any lower than that and overfitting becomes apparent, while higher and the gradient is not accurate enough.\n",
        "\n",
        "Because this model takes the average gradient and uses stochastic gradient descent, it takes a while to converge, so the learning rate had to be hightened.  Any lower than 0.4, and the gradient changes too slowly to approach a good optimized minimum, while too high and it tends to jump around before also settling on a sub-optimal value."
      ]
    },
    {
      "metadata": {
        "id": "sf1rK2ZPaejo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels_original)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# After much experimentation, I found that these parameters for batch size and learning rate give me the best results.\n",
        "batch_size = 32\n",
        "lr = 0.40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOaC-CjzxzXY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Softmax activation functions\n",
        "\n",
        "I created two softmax activation functions, one for use with multiple training examples, and one for just a single example.  The first activation function is used in batch training to handle all of the examples in the batch at once, while the second activation function is used in prediction of a number."
      ]
    },
    {
      "metadata": {
        "id": "0_k9Hm-aasyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def activation_batch(z):\n",
        "  # Calculate the exponent of each element.\n",
        "  e_batch = np.exp(z)\n",
        "  # Sum up all exponentiated elements for each example in the batch.\n",
        "  e_sum_batch = np.sum(e_batch,axis=1)\n",
        "  \n",
        "  # Normalize each element in the batch by their respective sum.\n",
        "  a = e_batch/e_sum_batch[:,None]\n",
        "  return a\n",
        "\n",
        "def activation(z):\n",
        "  # Just exponentiate all elements then divide each one by the sum of all exponents.\n",
        "  e = np.exp(z)\n",
        "  return e/np.sum(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XtHmT_vd00hX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss function\n",
        "\n",
        "Used in marking the loss over each epoch as we train."
      ]
    },
    {
      "metadata": {
        "id": "PFcSk24kH7vy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The negative dot of the categorical labels with the log of the activation results.\n",
        "def loss(a, y):\n",
        "  return -y.dot(np.log(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhP8G45P1RmU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating and training the classifier\n",
        "\n",
        "This function performs mini-batch gradient descent.  It returns the weight matrix and the vector of biases for each of the ten output neurons.  It also returns a list of loss averages over each epoch for plotting.\n"
      ]
    },
    {
      "metadata": {
        "id": "s3RJL7G9a3aW",
        "colab_type": "code",
        "outputId": "d6357862-eb13-4307-ca5c-1be52a9056d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "cell_type": "code",
      "source": [
        "def classifier():\n",
        "  # We have 10 output neurons, and each neuron has 28*28 weights coming in.\n",
        "  weights = np.random.randn(10, 784)\n",
        "  bias = np.random.randn(10)\n",
        "  \n",
        "  # The Kronecker Delta, used in calculating the partial derivatives, is just the identity matrix.\n",
        "  kronecker = np.identity(10)\n",
        "  \n",
        "  num_ims = len(train_images)\n",
        "  loss_avgs = []\n",
        "  # We train over a number of epochs.\n",
        "  for epoch in range(epochs):\n",
        "    # On each epoch, shuffle the training data.\n",
        "    shuffled_i = np.random.permutation(num_ims)\n",
        "    X = train_images[shuffled_i]\n",
        "    Y = train_labels[shuffled_i]\n",
        "    # Record the average loss for each epoch.\n",
        "    loss_epoch = 0.0\n",
        "    \n",
        "    # Take batches and perform gradient descent on them, using the average gradients\n",
        "    # to update the weight and bias.\n",
        "    for i in range(0, num_ims, batch_size):\n",
        "        x = X[i:i+batch_size]\n",
        "        y = Y[i:i+batch_size]\n",
        "        \n",
        "        # The last batch may be smaller than the batch-size.\n",
        "        num_x = len(y)\n",
        "        \n",
        "        # Find the weighted inputs by performing the dot product of each example with\n",
        "        # the weights and adding the biases.\n",
        "        z = x.dot(weights.T) + bias\n",
        "        \n",
        "        # Perform activation on the entire batch.\n",
        "        a = activation_batch(z)\n",
        "        \n",
        "        # The calculation of the partial derivatives is broken into 2 steps:\n",
        "        # 1. subtract the activation of each example from the kronecker delta.\n",
        "        a_min_k = np.apply_along_axis(np.subtract, 1, a, kronecker)\n",
        "        \n",
        "        gradient_b = np.empty((num_x,10))\n",
        "        gradient_w = np.empty((num_x,10,784))\n",
        "        \n",
        "        for k in range(num_x):\n",
        "          # 2. Dot the previous result with the categorical labels to get the gradient with\n",
        "          # respect to the biases for each example.\n",
        "          gradient_b[k] = np.dot(y[k], a_min_k[k])\n",
        "          # The gradient with respect to weights is just the gradient with respect to biases \n",
        "          # times the input.\n",
        "          gradient_w[k] = gradient_b[k][:,None] * x[k]\n",
        "          # Record the loss for each example.\n",
        "          loss_epoch += loss(a[k],y[k])\n",
        "        \n",
        "        # Average both gradients to use in update step.\n",
        "        gradient_b = np.mean(gradient_b, axis=0)\n",
        "        gradient_w = np.mean(gradient_w, axis=0)\n",
        "        \n",
        "        # Update the weights and biases and move to the next epoch.\n",
        "        weights = weights - lr * gradient_w\n",
        "        bias = bias - lr * gradient_b\n",
        "   \n",
        "    # Record the overall loss for the epoch\n",
        "    loss_avgs.append(loss_epoch/num_ims)\n",
        "    print(\"Epoch %d/%d.\\tLoss: %f\" % (epoch+1, epochs, loss_epoch/num_ims))\n",
        "  \n",
        "  return (weights, bias, loss_avgs)\n",
        "\n",
        "# Run the function to get the triple which represents the model.\n",
        "(weights, bias, loss_avgs) = classifier()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10.\tLoss: 0.891747\n",
            "Epoch 2/10.\tLoss: 0.482861\n",
            "Epoch 3/10.\tLoss: 0.416084\n",
            "Epoch 4/10.\tLoss: 0.383953\n",
            "Epoch 5/10.\tLoss: 0.359764\n",
            "Epoch 6/10.\tLoss: 0.343730\n",
            "Epoch 7/10.\tLoss: 0.332545\n",
            "Epoch 8/10.\tLoss: 0.323213\n",
            "Epoch 9/10.\tLoss: 0.315893\n",
            "Epoch 10/10.\tLoss: 0.310402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DnpPZbfb7pDc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing loss values\n",
        "\n",
        "I plot the loss averages obtained from the classifier. "
      ]
    },
    {
      "metadata": {
        "id": "_uymv8JP5qCs",
        "colab_type": "code",
        "outputId": "da54662d-9a58-4475-9407-bb452d4ee261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "cell_type": "code",
      "source": [
        "eps = range(1, epochs + 1)\n",
        "plt.plot(eps, loss_avgs, 'bo', label='Training loss')\n",
        "plt.title('Average Training Loss Per Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "H8xud0HH8UOX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing the classification model on test images.\n",
        "\n",
        "I create a prediction function which takes the weights and biases returned from the model and a test image.  The function then dots the image data with the weight matrix, adds the biases, and then passes the result to the singular activation function, and then selects the argmax of the vector.  This argmax is the predicted number."
      ]
    },
    {
      "metadata": {
        "id": "J2NX89A4zWPh",
        "colab_type": "code",
        "outputId": "c392ba47-1ef3-4a0b-db4b-7fd4c2129f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def predict(w, b, x):\n",
        "  z = x.dot(w.T) + b\n",
        "  return np.argmax(activation(z))\n",
        "\n",
        "# Create two lists, one for storing the number of times the classifier\n",
        "# correctly identified a number, and one for storing the number of times\n",
        "# that number appears in the test data.\n",
        "num_true = [0 for i in range(10)]\n",
        "num_amount = [0 for i in range(10)]\n",
        "\n",
        "# Test on all testing examples.\n",
        "for i in range(10000):\n",
        "  # Get the prediction and the true label.\n",
        "  y_pred = predict(weights,bias, test_images[i])\n",
        "  y_true = test_labels_original[i]\n",
        "  \n",
        "  # If the two are equal, record the success.\n",
        "  num_amount[y_true]+=1\n",
        "  if y_pred == y_true:\n",
        "    num_true[y_true] += 1\n",
        "\n",
        "# Print the accuracy of the classifier in recognizing each number, and print\n",
        "# the overall accuracy.\n",
        "for i in range(10): \n",
        "  print(\"Classifier's accuracy in recognizing %d: %f\" % (i, num_true[i]/num_amount[i]))\n",
        "\n",
        "print(\"\\nOverall Accuracy: %f\" % (sum(num_true)/10000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier's accuracy in recognizing 0: 0.977551\n",
            "Classifier's accuracy in recognizing 1: 0.978855\n",
            "Classifier's accuracy in recognizing 2: 0.916667\n",
            "Classifier's accuracy in recognizing 3: 0.881188\n",
            "Classifier's accuracy in recognizing 4: 0.929735\n",
            "Classifier's accuracy in recognizing 5: 0.843049\n",
            "Classifier's accuracy in recognizing 6: 0.949896\n",
            "Classifier's accuracy in recognizing 7: 0.919261\n",
            "Classifier's accuracy in recognizing 8: 0.856263\n",
            "Classifier's accuracy in recognizing 9: 0.900892\n",
            "\n",
            "Overall Accuracy: 0.916800\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}